import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{ah as h,an as t,ak as a,aj as n,ao as r,al as i,am as s,ai as d}from"./app-BVNKC5qK.js";const c={},u={id:"_4-百度贴吧内容备份",tabindex:"-1"},m={class:"header-anchor",href:"#_4-百度贴吧内容备份"},b={id:"_6-小红书内容备份",tabindex:"-1"},f={class:"header-anchor",href:"#_6-小红书内容备份"};function E(g,e){const p=s("Mermaid"),o=s("RouteLink");return d(),h("div",null,[e[4]||(e[4]=t("div",{class:"hint-container tip"},[t("p",{class:"hint-container-title"},"前言"),t("p",null,"备份这些大社交媒体内容，也算是给我们的青春做一个备份吧，怕这些社交媒体哪天说停运就停运了，丝毫不给我们回忆的机会。")],-1)),e[5]||(e[5]=t("h2",{id:"_1-方法思路",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#_1-方法思路"},[t("span",null,"1. 方法思路")])],-1)),e[6]||(e[6]=t("p",null,"总体来说，备份的思路是:",-1)),a(p,{id:"mermaid-11",code:"eJzT1dXlKsksyUm1Ung2dcuTvZOfNcx9sX09F0g8LSe/PDkjsahEwSeIi0sBCJ6u3/O0fZeurh1Q7fMJbUDG0/6JT3c0c3EBAFxMH4Q="}),e[7]||(e[7]=n('<h3 id="_1-1-导出" tabindex="-1"><a class="header-anchor" href="#_1-1-导出"><span>1.1 导出</span></a></h3><p>其中对我而言最困难的就是“导出”这个步骤，对于微博、QQ空间、小红书而言，可以借助现有的项目，实现相对容易的导出，对于贴吧而言，找了很久都没有找到合适的项目，后来几番寻找，找到了 <code>八爪鱼采集器</code>这个软件，在这个软件的帮助下，实现了数据的导出。</p><div class="hint-container note"><p class="hint-container-title">注</p><p>理论上 <code>八爪鱼采集器</code>也可以备份微博、QQ空间，但是现有的工具比较成熟了，也比较“傻瓜化”，不需要再白费力气琢磨如何从头开始获取数据。</p></div><h3 id="_1-2-整理" tabindex="-1"><a class="header-anchor" href="#_1-2-整理"><span>1.2 整理</span></a></h3><p>其实是对排版的整理，让初始的数据更好地在博客上面展示。而且，我想尽量原汁原味地保存下来，包括当时使用的表情符号、对应的链接跳转、图片等等，但是发现微博的很多表情符号都不再使用了，如 <code>[挖鼻屎]</code>这种。</p><h3 id="_1-3-发布" tabindex="-1"><a class="header-anchor" href="#_1-3-发布"><span>1.3 发布</span></a></h3><p>这个是相对简单的一部了，将已经处理好的内容，添加上<a href="https://theme-hope.vuejs.press/zh/config/frontmatter/info.html" target="_blank" rel="noopener noreferrer">Frontmatter</a>后就可以发布了。</p><h2 id="_2-个人qq空间内容备份" tabindex="-1"><a class="header-anchor" href="#_2-个人qq空间内容备份"><span>2. <a href="/Arthur/Qzone/%E8%AF%B4%E8%AF%B4">个人QQ空间内容备份</a></span></a></h2><p>(1) 通过下面这个项目，将所有信息导出md格式、原始图片信息</p><blockquote><p><a href="https://github.com/ShunCai/QZoneExport.html" target="_blank" rel="noopener noreferrer">ShunCai/QZoneExport: QQ空间导出助手</a></p></blockquote><p>(2) 将图片信息上传到NAS，借助在NAS部署的Alist转换为直链<br> (3) 使用vscode，手工将导出的md文件内的图片相对路径，批量修改为直链对应的链接</p><h2 id="_3-微博内容备份" tabindex="-1"><a class="header-anchor" href="#_3-微博内容备份"><span>3. <a href="/Arthur/Weibo">微博内容备份</a></span></a></h2><p>(1) 通过下面这个项目，将所有信息导出md格式、原始图片信息</p><blockquote><p><a href="https://github.com/Uchiha-Peng/weibo2markdown" target="_blank" rel="noopener noreferrer">Uchiha-Peng/weibo2markdown: 微博导出、微博备份、微博导出生成Markdown、微博爬虫 (github.com)</a></p></blockquote><p>(2) <s>获取的图片直链是不能直接访问的，需要借助 <code>https://image.baidu.com/search/down?url=图片地址</code>，将图片转换成可被识别的地址（到此md文件的图片就可以预览了，也可以通过3-4步骤下载图片自行保存）</s><br> 2023/9/11: 在我的建议下，项目更新了下载图片的功能，目前已经可以通过该项目便捷下载所有图片到本地。<br> (3) 通过浏览器插件/稳部落，将图片下载到本地，然后上传到NAS，借助在NAS部署的Alist转换为直链<br> (4) 使用vscode，手工将导出的md文件内的图片相对路径，批量修改为直链对应的链接</p>',15)),t("h2",u,[t("a",m,[t("span",null,[e[1]||(e[1]=r("4. ")),a(o,{to:"/Arthur/Tieba/"},{default:i(()=>e[0]||(e[0]=[r("百度贴吧内容备份")])),_:1})])])]),e[8]||(e[8]=n('<h3 id="_4-1-我的贴子列表、内容备份" tabindex="-1"><a class="header-anchor" href="#_4-1-我的贴子列表、内容备份"><span>4.1 我的贴子列表、内容备份</span></a></h3><p>各个贴子内容备份的难度比我想象中的要大，在github找了很久，都没有找到“开箱即用”的成熟的项目。后来找到一个单链接下载的python脚本，然后再借助CHATGPT，微调了一下代码，使其能支持批量下载（前提是先通过“列表备份”这一步骤导出所有需要备份的贴吧链接）。<br> 原项目：<a href="https://github.com/yuzequn095/tieba_crawler_tool/" target="_blank" rel="noopener noreferrer">yuzequn095/tieba_crawler_tool: Use Python3 to backup Baidu tieba content (github.com)</a></p><p>魔改后的项目：<a href="https://github.com/arthurfsy2/tieba_crawler_tool" target="_blank" rel="noopener noreferrer">arthurfsy2/tieba_crawler_tool: Use Python3 to backup Baidu tieba content (github.com)</a></p><p>2024/9/24更新：优化了原来的代码， 实现以下功能：</p><div class="hint-container important"><p class="hint-container-title">新功能</p><p>1、可批量处理贴子链接，并生成单独的markdown文件<br> 2、可批量下载图片<br> 3、可批量汇总“我的贴子”的列表、<a href="http://xn--list-9w8fl9ag06b699d8xya.md/merge.md" target="_blank" rel="noopener noreferrer">详细内容到list.md/merge.md</a></p></div><h2 id="_5-搜狗问问" tabindex="-1"><a class="header-anchor" href="#_5-搜狗问问"><span>5. <a href="/Arthur/%E6%90%9C%E7%8B%97%E9%97%AE%E9%97%AE/%E6%88%91%E7%9A%84%E5%9B%9E%E7%AD%94/list">搜狗问问</a></span></a></h2><h3 id="_5-1-列表备份" tabindex="-1"><a class="header-anchor" href="#_5-1-列表备份"><span>5.1 列表备份</span></a></h3><p><a href="/Arthur/%E6%90%9C%E7%8B%97%E9%97%AE%E9%97%AE/%E6%88%91%E7%9A%84%E6%8F%90%E9%97%AE/list">提问列表</a><br><a href="/Arthur/%E6%90%9C%E7%8B%97%E9%97%AE%E9%97%AE/%E6%88%91%E7%9A%84%E5%9B%9E%E7%AD%94/list">回答列表</a><br> 打开个人页，复制网页的提问/回答列表，直接粘贴到 <code>typora</code>当中。</p><h3 id="_5-2-提问-回答详情页备份" tabindex="-1"><a class="header-anchor" href="#_5-2-提问-回答详情页备份"><span>5.2 提问/回答详情页备份</span></a></h3><p>在github找了一圈，果然也没找到现成的项目，再次通过强大的ChatGPT写了一个脚本。<br><a href="https://github.com/arthurfsy2/sougouwenwen_crawler_tool" target="_blank" rel="noopener noreferrer">arthurfsy2/sougouwenwen_crawler_tool: 一个可以批量爬取指定搜狗问问问题链接，下载并转换为.md格式的python脚本。</a></p>',10)),t("h2",b,[t("a",f,[t("span",null,[e[3]||(e[3]=r("6. ")),a(o,{to:"/Arthur/XHS/"},{default:i(()=>e[2]||(e[2]=[r("小红书内容备份")])),_:1})])])]),e[9]||(e[9]=t("p",null,"自从我的新的自行车VOOK ONE 拿到手后，就嘚瑟地在小红书发了第一篇内容，后面都是发骑行相关的分享。",-1)),e[10]||(e[10]=t("p",null,"(1) 通过下面这个项目，在NAS上部署server服务，用来下载小红书的图片、视频",-1)),e[11]||(e[11]=t("blockquote",null,[t("p",null,[t("a",{href:"https://github.com/JoeanAmier/XHS-Downloader",target:"_blank",rel:"noopener noreferrer"},"JoeanAmier/XHS-Downloader: 小红书链接提取/作品采集工具：提取账号发布、收藏、点赞、专辑作品链接；提取搜索结果作品、用户链接；采集小红书作品信息；提取小红书作品下载地址；下载小红书无水印作品文件！")])],-1)),e[12]||(e[12]=t("p",null,"(2) 自己写了一个脚本，可以通过python脚本进行post请求获取到对应的作品数据，并转换为markdown文档",-1)),e[13]||(e[13]=t("blockquote",null,[t("p",null,"目前这个工具暂时还无法持续监测某个账号下已发布的作品列表，因此暂时无法实现自动更新和同步到blog的功能")],-1)),e[14]||(e[14]=t("blockquote",null,[t("p",null,[t("a",{href:"https://github.com/arthurfsy2/Blog_script",target:"_blank",rel:"noopener noreferrer"},"Blog_script")])],-1))])}const B=l(c,[["render",E]]),y=JSON.parse(`{"path":"/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/IT%E6%80%BB%E7%BB%93/others/%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E5%A4%87%E4%BB%BD.html","title":"社交媒体备份","lang":"zh-CN","frontmatter":{"icon":"edit","title":"社交媒体备份","date":"2023-09-02T00:00:00.000Z","category":["经验总结"],"tag":["IT总结"],"order":4,"description":"前言 备份这些大社交媒体内容，也算是给我们的青春做一个备份吧，怕这些社交媒体哪天说停运就停运了，丝毫不给我们回忆的机会。 1. 方法思路 总体来说，备份的思路是: 1.1 导出 其中对我而言最困难的就是“导出”这个步骤，对于微博、QQ空间、小红书而言，可以借助现有的项目，实现相对容易的导出，对于贴吧而言，找了很久都没有找到合适的项目，后来几番寻找，找到...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"社交媒体备份\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-09-02T00:00:00.000Z\\",\\"dateModified\\":\\"2024-11-29T03:26:30.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Mr.Arthur\\",\\"url\\":\\"https://blog.4a1801.life\\"}]}"],["meta",{"property":"og:url","content":"https://blog.4a1801.life/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/IT%E6%80%BB%E7%BB%93/others/%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E5%A4%87%E4%BB%BD.html"}],["meta",{"property":"og:site_name","content":"Family's Life"}],["meta",{"property":"og:title","content":"社交媒体备份"}],["meta",{"property":"og:description","content":"前言 备份这些大社交媒体内容，也算是给我们的青春做一个备份吧，怕这些社交媒体哪天说停运就停运了，丝毫不给我们回忆的机会。 1. 方法思路 总体来说，备份的思路是: 1.1 导出 其中对我而言最困难的就是“导出”这个步骤，对于微博、QQ空间、小红书而言，可以借助现有的项目，实现相对容易的导出，对于贴吧而言，找了很久都没有找到合适的项目，后来几番寻找，找到..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-11-29T03:26:30.000Z"}],["meta",{"property":"article:tag","content":"IT总结"}],["meta",{"property":"article:published_time","content":"2023-09-02T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-11-29T03:26:30.000Z"}]]},"git":{"createdTime":1693640818000,"updatedTime":1732850790000,"contributors":[{"name":"arthurfsy2","username":"arthurfsy2","email":"fsyflh@gmail.com","commits":12,"url":"https://github.com/arthurfsy2"}]},"readingTime":{"minutes":4.57,"words":1370},"filePathRelative":"经验总结/IT总结/others/社交媒体备份.md","localizedDate":"2023年9月2日","excerpt":"<div class=\\"hint-container tip\\">\\n<p class=\\"hint-container-title\\">前言</p>\\n<p>备份这些大社交媒体内容，也算是给我们的青春做一个备份吧，怕这些社交媒体哪天说停运就停运了，丝毫不给我们回忆的机会。</p>\\n</div>\\n<h2>1. 方法思路</h2>\\n<p>总体来说，备份的思路是:</p>\\n<h3>1.1 导出</h3>\\n<p>其中对我而言最困难的就是“导出”这个步骤，对于微博、QQ空间、小红书而言，可以借助现有的项目，实现相对容易的导出，对于贴吧而言，找了很久都没有找到合适的项目，后来几番寻找，找到了 <code>八爪鱼采集器</code>这个软件，在这个软件的帮助下，实现了数据的导出。</p>","autoDesc":true}`);export{B as comp,y as data};
