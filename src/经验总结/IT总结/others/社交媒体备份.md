---
icon: edit
title: 社交媒体备份
date: 2023-09-02
category:
  - 经验总结
tag:
  - IT总结
---
:::tip 前言
备份这些大社交媒体内容，也算是给我们的青春做一个备份吧，怕这些社交媒体哪天说停运就停运了，丝毫不给我们回忆的机会。
:::

## 方法思路

总体来说，备份的思路是:

```mermaid
---
title: 整体思路
---
flowchart LR

    导出-->整理-->发布

```

### 导出

其中对我而言最困难的就是“导出”这个步骤，对于微博、QQ空间而言，可以借助现有的项目，实现相对容易的导出，对于贴吧而言，找了很久都没有找到合适的项目，后来几番寻找，找到了 `八爪鱼采集器`这个软件，在这个软件的帮助下，实现了数据的导出。
:::note
理论上 `八爪鱼采集器`也可以备份微博、QQ空间，但是现有的工具比较成熟了，也比较“傻瓜化”，不需要再白费力气琢磨如何从头开始获取数据。
:::

### 整理

其实是对排版的整理，让初始的数据更好地在博客上面展示。而且，我想尽量原汁原味地保存下来，包括当时使用的表情符号、对应的链接跳转、图片等等，但是发现微博的很多表情符号都不再使用了，如 `[挖鼻屎]`这种。

### 发布

这个是相对简单的一部了，将已经处理好的内容，添加上[Frontmatter](https://theme-hope.vuejs.press/zh/config/frontmatter/info.html)后就可以发布了。

# 百度贴吧、微博、QQ空间、搜狗问问备份

## [个人QQ空间内容备份](/Arthur/Qzone)

1. 通过下面这个项目，将所有信息导出md格式、原始图片信息
   > [ShunCai/QZoneExport: QQ空间导出助手](https://github.com/ShunCai/QZoneExport.html)
   >
2. 将图片信息上传到onedrive网盘，借助在云服务端部署的alist转换为直链
3. 使用vscode，手工将导出的md文件内的图片相对路径，批量修改为直链对应的链接

## [微博内容备份](/Arthur/Weibo)

1. 通过下面这个项目，将所有信息导出md格式、原始图片信息
   > [Uchiha-Peng/weibo2markdown: 微博导出、微博备份、微博导出生成Markdown、微博爬虫 (github.com)](https://github.com/Uchiha-Peng/weibo2markdown)
   >
~~2. 获取的图片直链是不能直接访问的，需要借助 `https://image.baidu.com/search/down?url=图片地址`，将图片转换成可被识别的地址（到此md文件的图片就可以预览了，也可以通过3-4步骤下载图片自行保存）~~
   2023/9/11: 在我的建议下，项目更新了下载图片的功能，目前已经可以通过该项目便捷下载所有图片到本地。
3. 通过浏览器插件/稳部落，将图片下载到本地，然后上传到onedrive网盘，借助在云服务端部署的alist转换为直链
4. 使用vscode，手工将导出的md文件内的图片相对路径，批量修改为直链对应的链接

## [百度贴吧内容备份](/Arthur/Tieba)

### 列表备份

1. 下载八爪鱼采集器
   [免费下载-Windows大数据采集软件下载 - 八爪鱼采集器 (bazhuayu.com)](https://www.bazhuayu.com/download/windows)
2. 输入以下网址，确定导出的字段，采集完后导出为html格式

   [我的贴子_i贴吧 (baidu.com)](https://tieba.baidu.com/i/i/my_tie)

   [我回复的_i贴吧 (baidu.com)](https://tieba.baidu.com/i/i/my_reply)
3. 通过 [python代码](/经验总结/IT总结/实用工具汇总.html#html合并后转换为markdown)合并为一个html文件，并将该 `html `转换成 `markdown`格式
4. 手动调整markdown的内容，使其在vuepress主题下能正常展示

### 各贴子内容备份

各个贴子内容备份的难度比我想象中的要大，在github找了很久，都没有找到“开箱即用”的成熟的项目。后来找到一个单链接下载的python脚本，然后再借助CHATGPT，微调了一下代码，使其能支持批量下载（前提是先通过“列表备份”这一步骤导出所有需要备份的贴吧链接）。
原项目：[yuzequn095/tieba_crawler_tool: Use Python3 to backup Baidu tieba content (github.com)](https://github.com/yuzequn095/tieba_crawler_tool/)

魔改后的项目：[arthurfsy2/tieba_crawler_tool: Use Python3 to backup Baidu tieba content (github.com)](https://github.com/arthurfsy2/tieba_crawler_tool)

## [搜狗问问](/Arthur/搜狗问问)

### 列表备份

[提问列表](/Arthur/搜狗问问/我的提问/list)
[回答列表](/Arthur/搜狗问问/我的回答/list)
打开个人页，复制网页的提问/回答列表，直接粘贴到 `typora`当中。（贴吧不能通过这个办法估计是因为贴吧的列表有防爬策略）

### 提问/回答详情页备份

在github找了一圈，果然也没找到现成的项目，再次通过强大的ChatGPT写了一个脚本。
[arthurfsy2/sougouwenwen_crawler_tool: 一个可以批量爬取指定搜狗问问问题链接，下载并转换为.md格式的python脚本。](https://github.com/arthurfsy2/sougouwenwen_crawler_tool)
